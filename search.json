[
  {
    "objectID": "posts/llm-temperature/llm_temperature.html",
    "href": "posts/llm-temperature/llm_temperature.html",
    "title": "Understanding OpenAI ChatCompletion Model Parameters",
    "section": "",
    "text": "In this lesson, we will go through different parameters in LLM that control the token generation process\n\ntemperature\ntop_p\nfrequency_penalty\npresence_penalty\n\nI have written two helper functions highlight_openai_response and highlight for highlighting the probabilities of the tokens generated by the model. Less probable tokens are highlighted in white and more probable tokens are highlighted in green.\n\n\n\nIMG_F4CC52CE7F7F-1.jpeg\n\n\n\nquestion = \"\"\"\n\nWhat is machine learning? Explain it to a five year old.\nAnswer within 100 words, 3 paragraphs\n\"\"\"\n\nmodel = \"gpt-3.5-turbo\"\n\n\n\n\nimport seaborn as sns\nfrom IPython.display import HTML\nimport matplotlib.colors as mcolors\nimport numpy as np\n\n\ndef highlight_openai_response(response):\n    messages = response.choices[0].message.content\n    probabilities = []\n\n    for res in response.choices[0].logprobs.content:\n        probabilities.append(np.exp(res.logprob))\n\n    highlight(messages, probabilities)\n\n\ndef highlight(text, probabilities):\n    # Split the text into words, preserving newlines and spaces\n    words = []\n    for line in text.split(\"\\n\"):\n        words.extend([(word, \" \") for word in line.split(\" \")] + [(\"\\n\", \"\")])\n\n    # Remove the last element if it is a newline, added due to the split\n    if words[-1][0] == \"\\n\":\n        words.pop()\n\n    # Ensure probabilities list matches the number of non-empty words\n    normalized_probs = [min(max(0, p), 1) for p in probabilities]\n\n    # Use a Seaborn color palette and map probabilities to colors\n    palette = sns.light_palette(\"green\", as_cmap=True)\n\n    # Start building the HTML string using the 'pre' tag to preserve whitespace\n    html_string = \"&lt;pre style='font-family: inherit; white-space: pre-wrap; word-break: break-all;'&gt;\"\n\n\n    prob_index = 0  # Index for the current probability\n\n    for word, space in words:\n        if word and word != \"\\n\":  # If the element is not a space or newline\n            rgba_color = palette(normalized_probs[prob_index])\n            hex_color = mcolors.to_hex(rgba_color)\n            # Set the text color to black and the background color to the word's color\n            html_string += f\"&lt;span style='background-color: {hex_color}; color: black;'&gt;{word}&lt;/span&gt;\"\n            if space:\n                # Set the space's background color to the word's color\n                html_string += f\"&lt;span style='background-color: {hex_color}; color: black;'&gt;{space}&lt;/span&gt;\"\n            prob_index += 1\n        elif word == \"\\n\":\n            # Add a newline in HTML, and reset the color for the next line\n            html_string += \"&lt;br&gt;\"\n        else:\n            # This case handles multiple spaces in a row\n            previous_hex_color = mcolors.to_hex(\n                palette(normalized_probs[prob_index - 1])\n            )\n            html_string += (\n                f\"&lt;span style='background-color: {previous_hex_color};'&gt; &lt;/span&gt;\"\n            )\n\n    html_string += \"&lt;/pre&gt;\"  # Close the 'pre' tag\n\n    # Display the HTML string\n    display(HTML(html_string))\n\n\nhighlight(\"Hello I am Arun\", [0.9, 0.8, 0.6, 0.4])\n\nHello I am Arun \n\n\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n\nfrom openai import OpenAI, __version__\nprint(f\"OpenAI version: {__version__}\")\nclient = OpenAI()\n\nOpenAI version: 1.5.0\n\n\n\nseed = 42\n\n\nTokens\nIn large language models (LLMs), tokens are the smallest units of text that the model processes and generates. They can represent individual characters, words, subwords, or even larger linguistic units, depending on the specific tokenization approach used. Tokens act as a bridge between the raw text data and the numerical representations that LLMs can work with.\nIn the context of OpenAI, tokens are the basic units of text processed by their language models, such as GPT-3. OpenAI employs Byte-Pair Encoding (BPE) for tokenization, which is a method initially designed for text compression. BPE identifies the most frequent pairs of characters or tokens and merges them to form new tokens, thus optimizing the tokenization process for efficiency and effectiveness in representing the text data. This approach allows the model to handle a wide range of vocabulary, including rare words or phrases, by breaking them down into subword units.\n\n\n\nimage.png\n\n\nsource https://platform.openai.com/tokenizer\nIn openai chat completion APIs, four parameter controls the token generation process. They are\n\ntemperature\ntop_p\nfrequency_penalty\npresence_penalty\n\n\n\nTemperature\nIn large language models, temperature is a parameter that controls the randomness of predictions by scaling the logits before applying the softmax function. A low temperature makes the model more confident and conservative, favoring more likely predictions, while a high temperature increases diversity and creativity, allowing for less probable outcomes.\nTemperature adjusts the probability distribution of the next word. A higher temperature increases randomness, while a lower one makes the model more deterministic.\nPurpose: It controls the level of unpredictability in the output.\nThe temperature adjustment equation in LaTeX format is as follows:\n\\[\nP'(w_i) = \\frac{P(w_i)^{\\frac{1}{T}}}{\\sum_{j=1}^{V} P(w_j)^{\\frac{1}{T}}}\n\\]\nHere, \\(P(w_i)\\) is the original probability of the word \\(w_i\\), \\(T\\) is the temperature, \\(P'(w_i)\\) is the adjusted probability of the word, and \\(V\\) is the vocabulary size (the total number of words over which the probabilities are distributed). This equation shows how each original probability \\(P(w_i)\\) is raised to the power of the reciprocal of the temperature, and then normalized by dividing by the sum of all such adjusted probabilities to ensure that the adjusted probabilities sum to 1.\n\n0.15** (1/1.9)\n\n0.368437494723581\n\n\n\n0.15** (1/0.7)\n\n0.06652540281931184\n\n\n\nimport pandas as pd\n\n# Base probabilities for 20 words\nbase_probabilities = [\n    0.19, 0.12, 0.10, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03,\n    0.03, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01\n]\n\n# Temperatures\nhigh_temperature = 1.9\nlow_temperature = 0.4\n\n# Adjusted probabilities with high temperature\nadjusted_probabilities_high = [p ** (1 / high_temperature) for p in base_probabilities]\n\n# Adjusted probabilities with low temperature\nadjusted_probabilities_low = [p ** (1 / low_temperature) for p in base_probabilities]\n\n# Normalizing the adjusted probabilities for high temperature\nsum_adjusted_probabilities_high = sum(adjusted_probabilities_high)\nnormalized_probabilities_high = [p / sum_adjusted_probabilities_high for p in adjusted_probabilities_high]\n\n# Normalizing the adjusted probabilities for low temperature\nsum_adjusted_probabilities_low = sum(adjusted_probabilities_low)\nnormalized_probabilities_low = [p / sum_adjusted_probabilities_low for p in adjusted_probabilities_low]\n\nwords = [f\"word{i}\" for i in range(20)]\n\n# Create a DataFrame with the words and their probabilities, adjusted for high and low temperatures\ndf = pd.DataFrame({\n    \"word\": words,\n    \"base_probability\": base_probabilities,\n    \"adjusted_probability_high=1.9\": adjusted_probabilities_high,\n    \"normalized_probabilities_high=1.9\": normalized_probabilities_high,\n    \"adjusted_probability_low=0.4\": adjusted_probabilities_low,\n    \"normalized_probabilities_low=0.4\": normalized_probabilities_low\n})\n\ndf\n\n\n\n\n\n\n\n\nword\nbase_probability\nadjusted_probability_high=1.9\nnormalized_probabilities_high=1.9\nadjusted_probability_low=0.4\nnormalized_probabilities_low=0.4\n\n\n\n\n0\nword0\n0.19\n0.417250\n0.111183\n0.015736\n0.493728\n\n\n1\nword1\n0.12\n0.327611\n0.087298\n0.004988\n0.156515\n\n\n2\nword2\n0.10\n0.297635\n0.079310\n0.003162\n0.099221\n\n\n3\nword3\n0.09\n0.281580\n0.075032\n0.002430\n0.076245\n\n\n4\nword4\n0.08\n0.264654\n0.070522\n0.001810\n0.056797\n\n\n5\nword5\n0.07\n0.246693\n0.065736\n0.001296\n0.040677\n\n\n6\nword6\n0.06\n0.227469\n0.060613\n0.000882\n0.027668\n\n\n7\nword7\n0.05\n0.206656\n0.055067\n0.000559\n0.017540\n\n\n8\nword8\n0.04\n0.183756\n0.048965\n0.000320\n0.010040\n\n\n9\nword9\n0.03\n0.157937\n0.042085\n0.000156\n0.004891\n\n\n10\nword10\n0.03\n0.157937\n0.042085\n0.000156\n0.004891\n\n\n11\nword11\n0.03\n0.157937\n0.042085\n0.000156\n0.004891\n\n\n12\nword12\n0.02\n0.127587\n0.033998\n0.000057\n0.001775\n\n\n13\nword13\n0.02\n0.127587\n0.033998\n0.000057\n0.001775\n\n\n14\nword14\n0.02\n0.127587\n0.033998\n0.000057\n0.001775\n\n\n15\nword15\n0.01\n0.088587\n0.023605\n0.000010\n0.000314\n\n\n16\nword16\n0.01\n0.088587\n0.023605\n0.000010\n0.000314\n\n\n17\nword17\n0.01\n0.088587\n0.023605\n0.000010\n0.000314\n\n\n18\nword18\n0.01\n0.088587\n0.023605\n0.000010\n0.000314\n\n\n19\nword19\n0.01\n0.088587\n0.023605\n0.000010\n0.000314\n\n\n\n\n\n\n\n\ndf[\"base_probability\"].sum()\n\n1.0\n\n\nAs we can see that the base probabilities decrease progressively from word0 to word19, starting at 0.19 and going down to 0.01. However, after the adjustment, the probabilities are closer to each other, indicating that the temperature scaling has made the less likely words more probable and the more probable words less dominant.\nFor example, word0 has its probability decreased from 0.19 to about 0.11, while word19 has its probability slightly increased from 0.01 to about 0.024. This adjustment serves to flatten the probability distribution, making the model less certain and more explorative in its word choices.\nThe adjusted probabilities are also normalized, as their sum should equal 1 to represent a valid probability distribution. This adjustment allows for a less deterministic and more varied text generation, which can be useful for generating more diverse and creative text outputs.\nThe temperature adjustment has effectively reduced the likelihood of the most probable word being selected and increased the likelihood of less probable words, thus adding variability to the text generation process.\n\n# plot the probabilities\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nplt.plot(words, base_probabilities, label=\"Base Probabilities\")\nplt.plot(words, normalized_probabilities_high, label=\"High Temperature\")\nplt.plot(words, normalized_probabilities_low, label=\"Low Temperature\")\nplt.xticks(rotation=90)\nplt.xlabel(\"Words\")\nplt.ylabel(\"Probability\")\nplt.legend()\nplt.show()\n\n\n\n\n\nTemperature : 0( Deterministic)\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=0,\n  seed=seed\n)\n\n\nprint(response.choices[0].message.content)\n\nMachine learning is like having a super smart robot friend that can learn things on its own. Just like how you learn new things by practicing and trying different things, machine learning is when a computer program learns from lots of examples and gets better at doing a task. For example, if you show the robot friend lots of pictures of cats and dogs, it can learn to tell the difference between them. It's like magic, but really it's just the computer using math and patterns to figure things out.\n\n\n\nresponse.choices[0].logprobs.content[:5] # first 5 tokens\n\n[ChatCompletionTokenLogprob(token='Machine', bytes=[77, 97, 99, 104, 105, 110, 101], logprob=-0.001537835, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' learning', bytes=[32, 108, 101, 97, 114, 110, 105, 110, 103], logprob=-0.00058532227, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' is', bytes=[32, 105, 115], logprob=-0.00044044392, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' like', bytes=[32, 108, 105, 107, 101], logprob=-0.31134152, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' having', bytes=[32, 104, 97, 118, 105, 110, 103], logprob=-1.0659788, top_logprobs=[])]\n\n\n\nprobs = []\n\n\nimport numpy as np\n\nfor res in response.choices[0].logprobs.content:\n    probs.append(np.exp(res.logprob))\n\n\nplt.hist(probs);\nplt.xlabel(\"Probability\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\nresponse.system_fingerprint\n\n\nhighlight_openai_response(response)\n\nMachine learning is like having a super smart robot friend that can learn things on its own. Just like how you learn new things by practicing and trying different things, machine learning is when a computer program learns from lots of examples and gets better at doing a task. For example, if you show the robot friend lots of pictures of cats and dogs, it can learn to tell the difference between them. It's like magic, but really it's just the computer using math and patterns to figure things out. \n\n\n\n\n\nHigh Temperature ( More Randomness)\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1.6,\n  seed=seed\n)\n\n\nhighlight_openai_response(response)\n\nMachine learning is when computer programs can learn and improve by themselves.   Imagine you have a puzzle toy with pictures on it. At first it's all mixed up and you don't know how to make it right. When you start solving it a few times, you start getting better and can finish the puzzle faster. That's how machines work too, they start with learning puzzles and every time they make a mistake, they remember that and try not to make the same mistake again. They keep getting better and better! They can use what they have learned to help us, like find things on the internet or even drive cars. \n\n\n\nprobs = []\nfor res in response.choices[0].logprobs.content:\n    probs.append(np.exp(res.logprob))\n\nplt.hist(probs);\nplt.xlabel(\"Probability\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\nTemperature : 1 ( Default)\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed\n)\n\n\nhighlight_openai_response(response)\n\nMachine learning is when computers learn things on their own, just like how you learn new things. Imagine you have a special toy that can remember things it sees and hears. When you show it a picture of a cat and tell it \"this is a cat,\" the toy remembers that. Then, when you show it another picture of a cat without telling it anything, the toy can recognize that it's a cat too! This is because the toy learned from the first picture.   Machine learning is like that toy, but with big computers. They can learn from lots of pictures, sounds, and information to understand things all by themselves. The more they learn, the smarter they become. They can even help us do things like finding answers to questions, predicting what might happen next, and making decisions. It's like having a really smart friend who knows so many things! \n\n\n\nprobs = []\nfor res in response.choices[0].logprobs.content:\n    probs.append(np.exp(res.logprob))\n\nplt.hist(probs)\nplt.xlabel(\"Probability\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\nOpenAI Recommendations for Temperature\n - default is 1 - range: 0 to 2 \n\n\nTop-P (Nucleus Sampling)\nTop-p sampling, also known as nucleus sampling, is a technique used in large language models to control the diversity and quality of generated text. It involves selecting tokens from the most probable options, where the sum of their probabilities determines the selection.\nThe “top p” parameter acts as a filter, controlling how many different words or phrases the model considers when predicting the next word. The lower the value of p, the more deterministic the responses generated by the model are.\nThis method helps balance between diversity and high-probability words, ensuring the output is both diverse and contextually relevant.\n\ndf_p = df[['word', 'base_probability']].copy()\n\ndf_p['cumulative_probability'] = df_p['base_probability'].cumsum()\n\ndf_p\n\n\n\n\n\n\n\n\nword\nbase_probability\ncumulative_probability\n\n\n\n\n0\nword0\n0.19\n0.19\n\n\n1\nword1\n0.12\n0.31\n\n\n2\nword2\n0.10\n0.41\n\n\n3\nword3\n0.09\n0.50\n\n\n4\nword4\n0.08\n0.58\n\n\n5\nword5\n0.07\n0.65\n\n\n6\nword6\n0.06\n0.71\n\n\n7\nword7\n0.05\n0.76\n\n\n8\nword8\n0.04\n0.80\n\n\n9\nword9\n0.03\n0.83\n\n\n10\nword10\n0.03\n0.86\n\n\n11\nword11\n0.03\n0.89\n\n\n12\nword12\n0.02\n0.91\n\n\n13\nword13\n0.02\n0.93\n\n\n14\nword14\n0.02\n0.95\n\n\n15\nword15\n0.01\n0.96\n\n\n16\nword16\n0.01\n0.97\n\n\n17\nword17\n0.01\n0.98\n\n\n18\nword18\n0.01\n0.99\n\n\n19\nword19\n0.01\n1.00\n\n\n\n\n\n\n\n\ndf_p.style.apply(lambda x: ['background: yellow' if x.cumulative_probability &lt;= 0.8 else '' for i in x], axis=1)\n\n\n\n\n\n\n \nword\nbase_probability\ncumulative_probability\n\n\n\n\n0\nword0\n0.190000\n0.190000\n\n\n1\nword1\n0.120000\n0.310000\n\n\n2\nword2\n0.100000\n0.410000\n\n\n3\nword3\n0.090000\n0.500000\n\n\n4\nword4\n0.080000\n0.580000\n\n\n5\nword5\n0.070000\n0.650000\n\n\n6\nword6\n0.060000\n0.710000\n\n\n7\nword7\n0.050000\n0.760000\n\n\n8\nword8\n0.040000\n0.800000\n\n\n9\nword9\n0.030000\n0.830000\n\n\n10\nword10\n0.030000\n0.860000\n\n\n11\nword11\n0.030000\n0.890000\n\n\n12\nword12\n0.020000\n0.910000\n\n\n13\nword13\n0.020000\n0.930000\n\n\n14\nword14\n0.020000\n0.950000\n\n\n15\nword15\n0.010000\n0.960000\n\n\n16\nword16\n0.010000\n0.970000\n\n\n17\nword17\n0.010000\n0.980000\n\n\n18\nword18\n0.010000\n0.990000\n\n\n19\nword19\n0.010000\n1.000000\n\n\n\n\n\n\ndf_p[\"base_probability\"].sum()\n\n1.0\n\n\n\nHigh Top-P ( Includes more tokens to sample)\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  top_p=1,\n  seed=seed\n)\n\n\nprint(response.choices[0].message.content)\n\nMachine learning is when computers learn things on their own, just like how you learn new things. Imagine you have a special toy that can remember things it sees and hears. When you show it a picture of a cat and tell it \"this is a cat,\" the toy remembers that. Then, when you show it another picture of a cat without telling it anything, the toy can recognize that it's a cat too! This is because the toy learned from the first picture. \n\nMachine learning is like that toy, but with big computers. They can learn from lots of pictures, sounds, and information to understand things all by themselves. The more they learn, the smarter they become. They can even help us do things like finding answers to questions, predicting what might happen next, and making decisions. It's like having a really smart friend who knows so many things!\n\n\n\nhighlight_openai_response(response)\n\nMachine learning is when computers learn things on their own, just like how you learn new things. Imagine you have a special toy that can remember things it sees and hears. When you show it a picture of a cat and tell it \"this is a cat,\" the toy remembers that. Then, when you show it another picture of a cat without telling it anything, the toy can recognize that it's a cat too! This is because the toy learned from the first picture.   Machine learning is like that toy, but with big computers. They can learn from lots of pictures, sounds, and information to understand things all by themselves. The more they learn, the smarter they become. They can even help us do things like finding answers to questions, predicting what might happen next, and making decisions. It's like having a really smart friend who knows so many things! \n\n\n\nresponse.choices[0].logprobs.content[:5] # first 5 tokens\n\n[ChatCompletionTokenLogprob(token='Machine', bytes=[77, 97, 99, 104, 105, 110, 101], logprob=-0.0015492603, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' learning', bytes=[32, 108, 101, 97, 114, 110, 105, 110, 103], logprob=-0.0005857991, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' is', bytes=[32, 105, 115], logprob=-0.00047642877, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' when', bytes=[32, 119, 104, 101, 110], logprob=-1.7854097, top_logprobs=[]),\n ChatCompletionTokenLogprob(token=' computers', bytes=[32, 99, 111, 109, 112, 117, 116, 101, 114, 115], logprob=-0.25018257, top_logprobs=[])]\n\n\n\nprobs = []\nfor res in response.choices[0].logprobs.content:\n    probs.append(np.exp(res.logprob))\n\nplt.hist(probs);\nplt.xlabel(\"Probability\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nLow Top-P ( More Deterministic)\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  top_p=0.2,\n  seed=seed\n)\n\n\nhighlight_openai_response(response)\n\nMachine learning is like having a super smart robot friend that can learn things on its own. Just like how you learn new things by practicing and trying different things, machine learning is when a computer program learns from lots of examples and gets better at doing a task. For example, if you show the robot friend lots of pictures of cats and dogs, it can learn to tell the difference between them. It's like magic, but really it's just the computer using math and patterns to figure things out. \n\n\n\nprobs = []\nfor res in response.choices[0].logprobs.content:\n    probs.append(np.exp(res.logprob))\n\nplt.hist(probs);\nplt.xlabel(\"Probability\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\nOpenAI Recommendations for Top-P\n\n\n\nimage.png\n\n\n\n\n\nimage-2.png\n\n\n\n\nInteractions between Temperature and Top-P\nLet’s experiment the interactions between temperature and top-p\n\nHigh Temperature and High Top-P\nHigh Temperature and Low Top-P\nLow Temperature and High Top-P\nLow Temperature and Low Top-P\n\n\nHigh Temperature, High Top-P\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  top_p=1,\n  temperature=1.6,\n  seed=seed\n)\nhighlight_openai_response(response)\n\nMachine learning is when computer programs can learn and improve by themselves.   Imagine you have a puzzle toy with pictures on it. At first it's all mixed up and you don't know how to make it right. When you start solving it a few times, you start getting better and can finish the puzzle faster.   Machine learning works a bit like that. It's like a really smart program that learns from doing things over and over again. It gets more and more powerful, by learning from its own experiences! So just like you, the computer program gets better and better at solving the problems it faces. Isn’t that amazing?! \n\n\n\n\nHigh Temperature, Low Top-P\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  top_p=0.2,\n  temperature=1.5,\n  seed=seed\n)\nhighlight_openai_response(response)\n\nMachine learning is like having a super smart robot friend that can learn things on its own. Just like how you learn new things by practicing and trying different things, machine learning is when a computer program learns from lots of examples and gets better at doing a task. For example, if you show the robot friend lots of pictures of cats and dogs, it can learn to tell the difference between them. It's like magic, but really it's just the computer using math and patterns to figure things out. \n\n\n\n\nLow Temperature, High Top-P\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  top_p=1,\n  temperature=0.2,\n  seed=seed\n)\nhighlight_openai_response(response)\n\nMachine learning is like having a super smart robot friend that can learn things on its own. It's like when you play a game and get better each time because you remember what you did before. But instead of a game, the robot friend learns from lots of information and figures out patterns and rules. Then it can use what it learned to make predictions or do tasks without being told exactly what to do. It's like having a really clever friend who can help you with all sorts of things! \n\n\n\n\nLow Temperature, Low Top-P\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  top_p=0.2,\n  temperature=0.2,\n  seed=seed\n)\nhighlight_openai_response(response)\n\nMachine learning is like having a super smart robot friend that can learn things on its own. Just like how you learn new things by practicing and trying different things, machine learning is when a computer program learns from lots of examples and gets better at doing a task. For example, if you show the robot friend lots of pictures of cats and dogs, it can learn to tell the difference between them. It's like magic, but really it's just a computer using math to learn and make decisions. \n\n\n\n\n\nimage.png\n\n\n\n\n\nFrequency Penalty\nFrequency Penalty is used to reduce the likelihood of a token being selected again if it has already appeared in the generated text.\nIt ranges from -2.0 to 2.0, where positive values discourage repetition by penalizing tokens that occur frequently, and negative values can increase the likelihood of repetition. This helps control the diversity of the generated content and prevent verbatim repetition.\n\n\n\nimage.png\n\n\nIn the above example, we can see recommendations such as National Park appeared twice in the generated text. We can use frequency penalty to reduce the likelihood of a token being selected again if it has already appeared in the generated text.\n\nquestion = \"\"\"\n\nWrite 10 slogans for ChatGPT\n\n\"\"\"\n\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion!\" 3. \"Get Instant Answers with ChatGPT - Chat smarter, not harder!\" 4. \"Break the Ice with ChatGPT - The Ultimate Conversation Starter!\" 5. \"ChatGPT: With every chat, we'll wow you!\" 6. \"ChatGPT: Making Conversations Magical!\" 7. \"Experience Smarter Chats with ChatGPT - Your virtual chat guru!\" 8. \"Elevate Your Chats with ChatGPT - Your chatbot companion!\" 9. \"ChatGPT: The Perfect Balance of Wit and Intelligence!\" 10. \"Unlock the Potential of Chat with ChatGPT - Conversations made effortless!\" \n\n\n\nHigh Frequency Penalty\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed,\n  frequency_penalty=2\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion.\" 3. \"Get Instant Answers and Engaging Chats with ChatGPT!\" 4. \"Elevate Your Conversations with the Intelligence of ChatGPT.\" 5. \"Chat Smarter, With Confidence - Made Possible by ChatGPT!\" 6. \"Discover a New Level of Conversation Excellence with ChatGPT.\" 7. “Experience Artificial Intelligence that Feels Human – Meet chatbot G.” 8.“Make Every Interaction Count – Talk to Our Powerful AI Assistant!” 9.“Unlock Boundless Knowledge and Vivid Imagination– Say Hi to Our Intelligent AI friend!\"  10.\"Connect, Collaborate, Converse like never before - Powered by the Amazingness Of 'Yethe'\" \n\n\n\n\nLow Frequency Penalty\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed,\n  frequency_penalty=-0.5\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion!\" 3. \"Get Instant Answers with ChatGPT!\" 4. \"ChatGPT: Making Conversations Smarter!\" 5. \"Connect, Engage, and Learn with ChatGPT!\" 6. \"Elevate Your Conversations with ChatGPT!\" 7. \"ChatGPT: Your Virtual Conversational Superpower!\" 8. \"Experience the Future of Chat with ChatGPT!\" 9. \"ChatGPT: Making Talk as Intelligent as You!\" 10. \"ChatGPT: Your Chatbot Buddy for Every Occasion!\" \n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nPresence Penalty\nPresence Penalty is a parameter that influences the generation of new content by penalizing tokens that have already appeared in the text. It ranges from -2.0 to 2.0, where positive values discourage repetition and encourage the model to introduce new topics, while negative values do the opposite. This penalty is applied as a one-time, additive contribution to tokens that have been used at least once, helping to ensure more diverse and creative outputs\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion!\" 3. \"Get Instant Answers with ChatGPT - Chat smarter, not harder!\" 4. \"Break the Ice with ChatGPT - The Ultimate Conversation Starter!\" 5. \"ChatGPT: With every chat, knowledge expands!\" 6. \"Join the Chat Revolution - Welcome to ChatGPT!\" 7. \"Experience Chat Brilliance with ChatGPT - Seamless Conversations, Unmatched Results!\" 8. \"Chat Smarter, Talk Faster with ChatGPT!\" 9. \"ChatGPT: The Intelligent Chatbot for All Your Conversational Needs!\" 10. \"Unlock the Potential of Chat with ChatGPT - Conversations Redefined!\" \n\n\n\nHigh Presence Penalty\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed,\n  presence_penalty=1.5\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion.\" 3. \"Get Instant Answers and Engaging Chats with ChatGPT!\" 4. \"Elevate Your Conversations with ChatGPT's Intelligent AI.\" 5. \"ChatGPT: With You Every Step of the Conversation.\" 6. \"Unlock New Possibilities in Dialogue with ChatGPT.\" 7. \"Experience Natural Language Communication with ChatGPT.\" 8. \"Supercharge Your Conversations with ChatGPT's AI Assistant.\" 9. \"Chat Smarter, Not Harder, with ChatGPT.\" 10. \"Say Hello to Seamless Chats and Intelligent Responses with ChatGPT!\" \n\n\n\n\nLow Presence Penalty\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed,\n  presence_penalty=-2\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion!\" 3. \"ChatGPT: Chatting just got Smarter!\" 4. \"Connect with ChatGPT: Your Virtual Chatting Guru!\" 5. \"ChatGPT: Arm Your Conversations With Intelligence!\" 6. \"ChatGPT: Chatting Perfected with Artificial Intelligence!\" 7. \"ChatGPT: Your Personal Chatting Assistant with the Power of AI!\" 8. \"ChatGPT: Elevate Your Conversations to the Next Level!\" 9. \"ChatGPT: Your Smart Friend for Engaging Chats!\" 10. \"ChatGPT: Intelligent Conversations Made Effortless!\" \n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nInteraction between Frequency Penalty and Presence Penalty\n\nHigh Frequency Penalty and High Presence Penalty\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed,\n  presence_penalty=1.8,\n  frequency_penalty=1.8\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion.\" 3. \"Get Instant Answers and Engaging Chats with ChatGPT!\" 4. \"Elevate Your Conversations with AI-Powered ChatGPT.\" 5. \"Let's Talk! With Dynamic Dialogue Made Easy by ChatGPT.\" 6. \"Discover Smarter, More Natural Chats Using ChatGPT.\" 7. \"Unlock a World of Seamless Communication with ChatGPT.\"  8 .\"Experience Human-Like Interactions using our Advanced Assistant -Chat Gpt\" 9 .\"Your Virtual Conversation Buddy – Get Talking With Chat Gpt Now ! \" 10 .\"Revolutionize Your Conversations w \n\n\n\n\nLow Frequency Penalty and Low Presence Penalty\n\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": question}\n  ],\n  logprobs=True,\n  temperature=1,\n  seed=seed,\n  presence_penalty=-0.5,\n  frequency_penalty=-0.5\n)\nhighlight_openai_response(response)\n\n1. \"Unleash the Power of Chat with ChatGPT!\" 2. \"ChatGPT: Your Trusted Conversational Companion!\" 3. \"ChatGPT: Chatting just got Smarter!\" 4. \"Connect with ChatGPT: Chatting with Intelligence!\" 5. \"ChatGPT: Revolutionizing Chatting, One Conversation at a Time!\" 6. \"ChatGPT: Your Virtual Chatting Expert!\" 7. \"ChatGPT: Chatting with Artificial Intelligence that Feels Human!\" 8. \"ChatGPT: Chatting made Easy, Chatting made Powerful!\" 9. \"ChatGPT: Chatting with the Next Level of Chatbot Technology!\" 10. \"ChatGPT: Chatting. Redefined. \" \n\n\n\n\n\nParameter Value Suggestions\n\nIndustry 1: Creative Writing (e.g., Novels, Short Stories)\n\nTemperature: Set to 0.8-0.9. Higher temperature encourages more creative and unexpected turns of phrase, enhancing the storytelling with originality.\nTop P (Nucleus Sampling): Set around 0.9. Allows for a good range of probable words while still fostering creativity, which is vital in creative writing.\nFrequency Penalty: Set to a moderate value (e.g., 0.5). Helps avoid excessive repetition of words/phrases, maintaining a fresh and engaging narrative.\nPresence Penalty: Set to a lower value (e.g., 0.3-0.4). Encourages some repetition of key themes or phrases, which can be a powerful tool in storytelling.\n\n\n\nIndustry 2: Customer Support (e.g., Chatbots for Service Queries)\n\nTemperature: Set lower, around 0.3-0.4. Ensures more predictable and relevant responses, crucial for accurate customer support.\nTop P (Nucleus Sampling): Set around 0.8. Balances the need for coherent, relevant responses while allowing for some variability to better match customer queries.\nFrequency Penalty: Moderate to high (e.g., 0.6-0.8). In customer support, avoiding repetitive phrases can enhance clarity and professionalism in responses.\nPresence Penalty: Moderate (e.g., 0.5). Helps ensure a variety of information is provided, which can be crucial in addressing diverse customer queries comprehensively.\n\n\nExplanation:\n\nCreative Writing: The settings are designed to maximize creativity and originality, ensuring a rich and engaging narrative.\nCustomer Support: The focus here is on accuracy, relevance, and clarity in responses, which are essential in a customer support context."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Large Language Models - Explained Intuitively",
    "section": "",
    "text": "Understanding OpenAI ChatCompletion Model Parameters\n\n\n\n\n\n\n\nllm\n\n\nopenai\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nArun Prakash\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I write and teach about my learnings in large language models."
  }
]